# --- HACK: Workaround for Streamlit Cloud SQLite version ----
# Must be first import
import sys
__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# --- End of HACK ---
import streamlit as st
import openai
import os
import time
from utils import log_info, log_error, load_knowledge_base, split_text_into_chunks
from vector_db_manager import initialize_vector_db, add_chunks_to_vector_db, collection as chroma_collection_global
from rag_core import generate_response_with_rag
from config import * # Import t·∫•t c·∫£ c·∫•u h√¨nh

# --- Page Configuration (N√™n ƒë·∫∑t ·ªü ƒë·∫ßu) ---
st.set_page_config(
    page_title="Chatbot Ph√¢n t√≠ch ƒê·∫ßu t∆∞",
    page_icon="ü§ñ",
    layout="centered" # C√≥ th·ªÉ d√πng "wide" n·∫øu mu·ªën r·ªông h∆°n
)

# --- Logging ---
log_info("--- Kh·ªüi ƒë·ªông ·ª©ng d·ª•ng Chatbot Streamlit ---")

# --- Authentication & OpenAI Client Initialization ---
openai_client = None
openai_api_key = None

# C·ªë g·∫Øng l·∫•y API key t·ª´ Streamlit secrets (∆∞u ti√™n khi deploy)
try:
    openai_api_key = st.secrets.get("OPENAI_API_KEY")
except Exception as e:
    log_warning(f"Kh√¥ng th·ªÉ truy c·∫≠p st.secrets (c√≥ th·ªÉ ƒëang ch·∫°y local kh√¥ng c√≥ secrets): {e}")

# N·∫øu kh√¥ng c√≥ trong secrets, th·ª≠ l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (cho local test)
if not openai_api_key:
    openai_api_key = os.environ.get("OPENAI_API_KEY")
    if openai_api_key:
        log_info("ƒê√£ t√¨m th·∫•y OPENAI_API_KEY trong bi·∫øn m√¥i tr∆∞·ªùng.")

if not openai_api_key:
    st.error("L·ªói c·∫•u h√¨nh: Kh√¥ng t√¨m th·∫•y OpenAI API Key. Vui l√≤ng thi·∫øt l·∫≠p trong Streamlit Secrets (khi deploy) ho·∫∑c bi·∫øn m√¥i tr∆∞·ªùng (khi ch·∫°y local).")
    log_error("Thi·∫øu OpenAI API Key ƒë·ªÉ kh·ªüi ch·∫°y ·ª©ng d·ª•ng.")
    st.stop() # D·ª´ng ·ª©ng d·ª•ng

# Kh·ªüi t·∫°o OpenAI client
try:
    openai.api_key = openai_api_key # G√°n key cho th∆∞ vi·ªán (m·ªôt s·ªë h√†m c≈© c√≥ th·ªÉ c·∫ßn)
    openai_client = openai.OpenAI(api_key=openai_api_key)
    # Ki·ªÉm tra client b·∫±ng c√°ch g·ªçi m·ªôt l·ªánh nh·∫π
    openai_client.models.list()
    log_info("Kh·ªüi t·∫°o v√† x√°c th·ª±c OpenAI client th√†nh c√¥ng.")
except openai.AuthenticationError:
    st.error("L·ªói x√°c th·ª±c OpenAI API Key. Key kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ h·∫øt h·∫°n.")
    log_error("L·ªói x√°c th·ª±c OpenAI API Key.")
    st.stop()
except Exception as e:
    st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi kh·ªüi t·∫°o OpenAI client: {e}")
    log_error(f"L·ªói kh·ªüi t·∫°o OpenAI client: {e}", e)
    st.stop()


# --- Vector Database Setup and Indexing ---
# S·ª≠ d·ª•ng cache_resource ƒë·ªÉ tr√°nh kh·ªüi t·∫°o v√† indexing l·∫°i m·ªói l·∫ßn script ch·∫°y l·∫°i (do t∆∞∆°ng t√°c UI)
@st.cache_resource(show_spinner="ƒêang thi·∫øt l·∫≠p c∆° s·ªü d·ªØ li·ªáu vector...")
def setup_vector_database(api_key_for_embedding):
    """
    H√†m ƒë∆∞·ª£c cache ƒë·ªÉ kh·ªüi t·∫°o v√† n·∫°p d·ªØ li·ªáu v√†o Vector DB.
    Ch·ªâ ch·∫°y l·∫°i khi cache b·ªã x√≥a ho·∫∑c l·∫ßn ƒë·∫ßu kh·ªüi ƒë·ªông.
    """
    log_info("B·∫Øt ƒë·∫ßu ch·∫°y h√†m setup_vector_database (c√≥ th·ªÉ ƒë∆∞·ª£c cache)...")

    # 1. Kh·ªüi t·∫°o Vector DB collection
    # Truy·ªÅn API key v√†o ƒë√¢y ƒë·ªÉ ChromaDB t·∫°o OpenAIEmbeddingFunction
    initialized_collection = initialize_vector_db(api_key=api_key_for_embedding)

    if initialized_collection is None:
        log_error("L·ªói nghi√™m tr·ªçng: Kh√¥ng th·ªÉ kh·ªüi t·∫°o Vector Database collection.")
        # Kh√¥ng th·ªÉ hi·ªÉn th·ªã l·ªói tr·ª±c ti·∫øp t·ª´ h√†m cache, c·∫ßn x·ª≠ l√Ω b√™n ngo√†i
        return None # Tr·∫£ v·ªÅ None ƒë·ªÉ b√°o l·ªói

    # 2. Ki·ªÉm tra v√† N·∫°p d·ªØ li·ªáu (Indexing) n·∫øu c·∫ßn
    try:
        # S·ª≠ d·ª•ng collection tr·∫£ v·ªÅ t·ª´ h√†m kh·ªüi t·∫°o
        if initialized_collection.count() == 0:
            log_info(f"Collection '{COLLECTION_NAME}' r·ªóng. B·∫Øt ƒë·∫ßu Indexing...")

            # T·∫£i Knowledge Base
            knowledge_text = load_knowledge_base()
            if not knowledge_text:
                log_error("L·ªói nghi√™m tr·ªçng: Kh√¥ng th·ªÉ t·∫£i Knowledge Base ƒë·ªÉ indexing.")
                return None

            # Chia chunks
            chunks = split_text_into_chunks(knowledge_text)
            if not chunks:
                log_error("L·ªói nghi√™m tr·ªçng: Kh√¥ng th·ªÉ chia Knowledge Base th√†nh chunks.")
                return None

            # Th√™m v√†o Vector DB
            log_info(f"B·∫Øt ƒë·∫ßu th√™m {len(chunks)} chunks v√†o Vector DB (qu√° tr√¨nh n√†y c·∫ßn g·ªçi API embedding)...")
            # H√†m add_chunks_to_vector_db d√πng bi·∫øn collection to√†n c·ª•c,
            # n√™n c·∫ßn ƒë·∫£m b·∫£o bi·∫øn ƒë√≥ ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t trong initialize_vector_db
            # Ho·∫∑c t·ªët h∆°n l√† s·ª≠a add_chunks_to_vector_db ƒë·ªÉ nh·∫≠n collection l√†m tham s·ªë
            success = add_chunks_to_vector_db(chunks) # G·ªçi h√†m th√™m chunks
            if not success:
                log_error("L·ªói nghi√™m tr·ªçng: X·∫£y ra l·ªói trong qu√° tr√¨nh th√™m chunks v√†o Vector DB.")
                return None
            log_info(">>> Indexing d·ªØ li·ªáu v√†o Vector Database th√†nh c√¥ng! <<<")
        else:
            count = initialized_collection.count()
            log_info(f"Collection '{COLLECTION_NAME}' ƒë√£ c√≥ d·ªØ li·ªáu ({count} documents). B·ªè qua Indexing.")

        # G√°n collection ƒë√£ kh·ªüi t·∫°o v√†o bi·∫øn to√†n c·ª•c ƒë·ªÉ c√°c h√†m kh√°c d√πng (n·∫øu c·∫ßn)
        # L∆∞u √Ω: C√°ch n√†y kh√¥ng l√Ω t∆∞·ªüng l·∫Øm, nh∆∞ng ƒë∆°n gi·∫£n cho c·∫•u tr√∫c hi·ªán t·∫°i
        import vector_db_manager
        vector_db_manager.collection = initialized_collection

        return initialized_collection # Tr·∫£ v·ªÅ collection ƒë√£ s·∫µn s√†ng

    except Exception as e:
         log_error(f"L·ªói x·∫£y ra trong qu√° tr√¨nh ki·ªÉm tra ho·∫∑c indexing d·ªØ li·ªáu: {e}", e)
         return None # Tr·∫£ v·ªÅ None n·∫øu c√≥ l·ªói

# G·ªçi h√†m setup (Streamlit s·∫Ω qu·∫£n l√Ω cache)
chroma_collection_cached = setup_vector_database(openai_api_key)

# Ki·ªÉm tra k·∫øt qu·∫£ t·ª´ h√†m setup
if chroma_collection_cached is None:
    st.error("L·ªói nghi√™m tr·ªçng khi thi·∫øt l·∫≠p Vector Database. Kh√¥ng th·ªÉ kh·ªüi ch·∫°y chatbot. Vui l√≤ng ki·ªÉm tra log.")
    log_error("D·ª´ng ·ª©ng d·ª•ng do l·ªói setup Vector DB.")
    st.stop()
else:
    log_info("Vector Database ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p v√† s·∫µn s√†ng.")


# --- Streamlit Chat Interface ---
st.title("ü§ñ Chatbot Ph√¢n t√≠ch ƒê·∫ßu t∆∞")
st.caption(f"Tr·ª£ l√Ω AI tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu v·ªÅ C√¥ng ty FPT") # Nh·ªõ s·ª≠a l·∫°i

# Kh·ªüi t·∫°o l·ªãch s·ª≠ chat trong session state n·∫øu ch∆∞a c√≥
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": ASSISTANT_DEFAULT_MESSAGE, "avatar": ASSISTANT_AVATAR}]
    log_info("Kh·ªüi t·∫°o l·ªãch s·ª≠ chat m·ªõi.")

# Hi·ªÉn th·ªã c√°c tin nh·∫Øn ƒë√£ c√≥ trong l·ªãch s·ª≠
for message in st.session_state.messages:
    avatar = message.get("avatar")
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# Nh·∫≠n input m·ªõi t·ª´ ng∆∞·ªùi d√πng
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y..."):
    log_info(f"Nh·∫≠n c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng: {prompt}")
    # Th√™m tin nh·∫Øn c·ªßa user v√†o l·ªãch s·ª≠ v√† hi·ªÉn th·ªã ngay
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # T·∫°o v√† hi·ªÉn th·ªã ph·∫£n h·ªìi t·ª´ assistant
    with st.chat_message("assistant", avatar=ASSISTANT_AVATAR):
        message_placeholder = st.empty() # T·∫°o placeholder ƒë·ªÉ c·∫≠p nh·∫≠t t·ª´ t·ª´
        full_response = ""
        try:
            # Hi·ªÉn th·ªã spinner trong khi ch·ªù ph·∫£n h·ªìi
            with st.spinner("Chatbot ƒëang suy nghƒ©..."):
                 # G·ªçi h√†m RAG ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
                 # ƒê·∫£m b·∫£o openai_client ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng ·ªü tr√™n
                ai_response = generate_response_with_rag(openai_client, prompt)

            # Hi·ªáu ·ª©ng g√µ ch·ªØ (streaming simulation)
            response_words = ai_response.split()
            for i, word in enumerate(response_words):
                full_response += word + " "
                time.sleep(0.05) # Delay nh·ªè t·∫°o hi·ªáu ·ª©ng
                message_placeholder.markdown(full_response + ("‚ñå" if i < len(response_words) - 1 else "")) # Th√™m con tr·ªè nh·∫•p nh√°y
            message_placeholder.markdown(full_response) # Hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß khi xong
            log_info(f"Ph·∫£n h·ªìi c·ªßa Assistant (t√≥m t·∫Øt): {full_response[:100]}...")

        except Exception as e:
            full_response = DEFAULT_ERROR_MESSAGE # L·∫•y th√¥ng b√°o l·ªói m·∫∑c ƒë·ªãnh
            st.error(f"ƒê√£ x·∫£y ra l·ªói khi t·∫°o ph·∫£n h·ªìi: {e}") # Hi·ªÉn th·ªã l·ªói tr√™n UI
            log_error(f"L·ªói nghi√™m tr·ªçng khi t·∫°o ph·∫£n h·ªìi cho c√¢u h·ªèi '{prompt}': {e}", e)
            message_placeholder.markdown(full_response) # Hi·ªÉn th·ªã th√¥ng b√°o l·ªói

    # Th√™m ph·∫£n h·ªìi (ho·∫∑c l·ªói) c·ªßa assistant v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "assistant", "content": full_response, "avatar": ASSISTANT_AVATAR})

log_info("--- K·∫øt th√∫c m·ªôt l∆∞·ª£t x·ª≠ l√Ω request Streamlit ---")
